--- upstream/crypto_core/mult3sntrup857/avx/mult1024.c
+++ upstream-patched/crypto_core/mult3sntrup857/avx/mult1024.c
@@ -15,6 +15,26 @@
 #define mulhrs_x16 _mm256_mulhrs_epi16
 #define signmask_x16(x) _mm256_srai_epi16((x),15)
 
+typedef union {
+  int16 v[512];
+  int16x16 _dummy;
+} vec512;
+
+typedef union {
+  int16 v[8][512];
+  int16x16 _dummy;
+} vec8x512;
+
+typedef union {
+  int16 v[1024];
+  int16x16 _dummy;
+} vec1024;
+
+typedef union {
+  int16 v[4*512];
+  int16x16 _dummy;
+} vec2048;
+
 static int16x16 squeeze_3_x16(int16x16 x)
 {
   return sub_x16(x,mullo_x16(mulhrs_x16(x,const_x16(10923)),const_x16(3)));
@@ -69,9 +89,10 @@
     store_x16(&fpad[3][j],f3);
   }
 
-  for (i = 0;i < 4;++i)
+  for (i = 0;i < 4;++i) {
     for (j = 256;j < 512;++j)
       fpad[i][j] = 0;
+  }
 }
 
 static void unstride(int16 f[2048],const int16 fpad[4][512])
@@ -106,25 +127,56 @@
   }
 }
 
-#define ALIGNED __attribute((aligned(32)))
-
-static const ALIGNED int16 y_7681[512] = {
-#include "precomp7681.inc"
+static const vec512 y_7681 = { .v = {
+-3593,3364,1701,2237,2194,2557,834,-1599,514,438,2555,-1887,103,1881,-549,-1738,
+-3777,-2830,1414,1986,2456,1525,2495,-1993,2956,-679,2440,-810,2804,3555,1535,-3689,
+-3625,617,2319,-2816,3696,-1483,-2250,3706,-1399,-1760,2535,638,2043,396,2310,-3600,
+3182,-1921,2876,-2088,-1100,-1296,121,2006,-1321,-1305,-3772,-7,-1431,3174,3153,3266,
+3593,-3364,-1701,-2237,-2194,-2557,-834,1599,-514,-438,-2555,1887,-103,-1881,549,1738,
+3777,2830,-1414,-1986,-2456,-1525,-2495,1993,-2956,679,-2440,810,-2804,-3555,-1535,3689,
+3625,-617,-2319,2816,-3696,1483,2250,-3706,1399,1760,-2535,-638,-2043,-396,-2310,3600,
+-3182,1921,-2876,2088,1100,1296,-121,-2006,1321,1305,3772,7,1431,-3174,-3153,-3266,
+-1532,-3816,783,-921,-2160,2762,3310,727,2789,-373,-3456,-1497,-2385,-2391,-2426,2883,
+1919,2233,-1056,2743,-2649,3750,-1168,1521,2919,-2175,-1166,-2572,-3405,-660,3831,-1681,
+404,-2764,1799,1386,-859,1390,-2133,-1464,-194,-3692,-1054,-1350,2732,3135,-915,2224,
+-486,-2835,2665,3428,-2579,1598,-3480,1533,-3417,-730,-1698,3145,2113,-1756,-2,-3588,
+1532,3816,-783,921,2160,-2762,-3310,-727,-2789,373,3456,1497,2385,2391,2426,-2883,
+-1919,-2233,1056,-2743,2649,-3750,1168,-1521,-2919,2175,1166,2572,3405,660,-3831,1681,
+-404,2764,-1799,-1386,859,-1390,2133,1464,194,3692,1054,1350,-2732,-3135,915,-2224,
+486,2835,-2665,-3428,2579,-1598,3480,-1533,3417,730,1698,-3145,-2113,1756,2,3588,
+-1404,-509,-1689,-3752,3335,2812,-1519,1669,-402,-2345,2963,370,3745,83,-796,642,
+-2874,-1403,777,3677,-1084,-3763,-188,692,-429,1338,124,-293,3366,-3408,3163,-1837,
+1012,3343,-2262,-2460,-2532,592,893,-3287,1931,2303,-3208,-2083,3214,826,2258,2965,
+-2130,2937,-2070,-3657,-1441,-2005,2386,2167,3723,2515,589,-3312,-3334,-1526,-3781,-791,
+1404,509,1689,3752,-3335,-2812,1519,-1669,402,2345,-2963,-370,-3745,-83,796,-642,
+2874,1403,-777,-3677,1084,3763,188,-692,429,-1338,-124,293,-3366,3408,-3163,1837,
+-1012,-3343,2262,2460,2532,-592,-893,3287,-1931,-2303,3208,2083,-3214,-826,-2258,-2965,
+2130,-2937,2070,3657,1441,2005,-2386,-2167,-3723,-2515,-589,3312,3334,1526,3781,791,
+658,1278,-226,1649,-436,17,-1181,1242,-3434,451,-3770,3581,2719,1779,-1144,-1509,
+-1476,-929,3542,2161,-236,3744,-1203,179,-3550,-2786,-3450,1586,-3461,-3547,1072,2918,
+-715,2230,2767,2072,1121,-2422,3794,1070,2891,222,1295,3568,2998,-434,2589,-2339,
+670,1348,-2378,-3177,-2071,2001,151,2059,2340,-1712,2815,3693,3314,-1151,2247,-1407,
+-658,-1278,226,-1649,436,-17,1181,-1242,3434,-451,3770,-3581,-2719,-1779,1144,1509,
+1476,929,-3542,-2161,236,-3744,1203,-179,3550,2786,3450,-1586,3461,3547,-1072,-2918,
+715,-2230,-2767,-2072,-1121,2422,-3794,-1070,-2891,-222,-1295,-3568,-2998,434,-2589,2339,
+-670,-1348,2378,3177,2071,-2001,-151,-2059,-2340,1712,-2815,-3693,-3314,1151,-2247,1407,
+  }
 } ;
 
 static void mult1024(int16 h[2048],const int16 f[1024],const int16 g[1024])
 {
-  ALIGNED int16 fgpad[8][512];
-#define fpad fgpad
-#define gpad (fgpad+4)
+  vec8x512 fgpad;
+#define fpad (fgpad.v)
+#define gpad (fgpad.v+4)
 #define hpad fpad
-  ALIGNED int16 h_7681[2048];
+  vec2048 aligned_h_7681;
+#define h_7681 (aligned_h_7681.v)
   int i;
 
   stride(fpad,f);
   stride(gpad,g);
 
-  ntt512_7681(fgpad[0],8);
+  ntt512_7681(fgpad.v[0],8);
 
   /* XXX: try arbitrary-degree Karatsuba */
 
@@ -159,7 +211,7 @@
     int16x16 h4 = sub_x16(d1d2d3,e13);
     int16x16 h5 = sub_x16(d2d3,e23);
     int16x16 h6 = d3;
-    int16x16 twist = load_x16(&y_7681[i]);
+    int16x16 twist = load_x16(&y_7681.v[i]);
     h4 = mulmod_7681_x16(h4,twist);
     h5 = mulmod_7681_x16(h5,twist);
     h6 = mulmod_7681_x16(h6,twist);
@@ -173,7 +225,7 @@
   }
 
   invntt512_7681(hpad[0],4);
-  unstride(h_7681,hpad);
+  unstride(h_7681,(const int16(*)[512]) hpad);
 
   for (i = 0;i < 2048;i += 16) {
     int16x16 u = load_x16(&h_7681[i]);
@@ -202,9 +254,11 @@
 
 int crypto_core(unsigned char *outbytes,const unsigned char *inbytes,const unsigned char *kbytes,const unsigned char *cbytes)
 {
-  ALIGNED int16 f[1024];
-  ALIGNED int16 g[1024];
-  ALIGNED int16 fg[2048];
+  vec1024 x1, x2;
+  vec2048 x3;
+#define f (x1.v)
+#define g (x2.v)
+#define fg (x3.v)
 #define h f
   int i;
   int16x16 x;
@@ -214,19 +268,19 @@
   for (i = p&~15;i < 1024;i += 16) store_x16(&g[i],x);
 
   for (i = 0;i < p;++i) {
-    int8 fi = inbytes[i];
+    int8 fi = (int8) inbytes[i];
     int8 fi0 = fi&1;
-    f[i] = fi0-(fi&(fi0<<1));
+    f[i] = (int16) (fi0-(fi&(fi0<<1)));
   }
   for (i = 0;i < p;++i) {
-    int8 gi = kbytes[i];
+    int8 gi = (int8) kbytes[i];
     int8 gi0 = gi&1;
-    g[i] = gi0-(gi&(gi0<<1));
+    g[i] = (int16) (gi0-(gi&(gi0<<1)));
   }
 
   mult1024(fg,f,g);
 
-  fg[0] -= fg[p-1];
+  fg[0] = (int16) (fg[0] - fg[p-1]);
   for (i = 0;i < 1024;i += 16) {
     int16x16 fgi = load_x16(&fg[i]);
     int16x16 fgip = load_x16(&fg[i + p]);
@@ -236,7 +290,7 @@
     store_x16(&h[i],x);
   }
   
-  for (i = 0;i < p;++i) outbytes[i] = h[i];
+  for (i = 0;i < p;++i) outbytes[i] = (unsigned char) h[i];
 
   return 0;
 }

